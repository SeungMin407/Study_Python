# 🌳 의사결정나무(Decision Tree) - 분류 & 회귀 정리
<img width="924" height="352" alt="image" src="https://github.com/user-attachments/assets/8947c28b-7a89-4232-a532-e3bac4f80e55" />

---

## 1. 의사결정나무란?
데이터를 여러 기준에 따라 반복적으로 분기(split)하며  
최종적으로 **클래스(분류)** 또는 **숫자값(회귀)** 을 예측하는 모델.

---

# 2. 의사결정나무 - 분류(Classification Tree)

## 목적
범주형 클래스(카테고리)를 예측하는 모델  
예: 스팸/정상메일, 구매/미구매, 병 있음/없음 등

## 분기 기준(불순도)
데이터가 “얼마나 순수하게 나뉘는지(Purity)”를 기준으로 분기함.

### ✔ Gini 불순도
- 0에 가까울수록 한 가지 클래스만 존재  
- 가장 많이 사용되는 기준

### ✔ Entropy(엔트로피)
- 정보 이론 기반의 불확실성 척도  
- Information Gain 최대화하는 분기 선택

---

## 출력 (리프 노드)
- 가장 많이 등장한 클래스  
- 또는 클래스 확률  
예: 고양이 0.8, 개 0.2 → “고양이”

---

## 분류 예시

나이 < 30?

├─ 예 → 인터넷 사용시간 > 3h → 구매 / 미구매

└─ 아니오 → 과거 구매 여부 → 구매 / 미구매


---

# 📙 3. 의사결정나무 - 회귀(Regression Tree)

## 목적
연속적인 숫자값을 예측  
예: 집값, 온도, 매출, 연비 등

## 분기 기준(오차 기반)

### ✔ MSE (Mean Squared Error)
- 평균 제곱 오차가 최소화되도록 분기  
- 가장 일반적인 회귀 기준

### ✔ MAE (Mean Absolute Error)
- 절댓값 오차 기준

👉 분류는 “불순도” 기반,  
👉 회귀는 “오차 감소” 기반이라는 점이 핵심 차이.

---

## 출력 (리프 노드)
해당 리프 노드에 속한 값들의 **평균값(또는 중앙값)**  
예: [3.0억, 3.2억, 2.9억] → 평균 3.03억

---

## 회귀 예시

평수 < 60?

├─ 예 → 지하철 가까움? → (평균 집값 출력)

└─ 아니오 → 방 개수로 분기 → (평균 집값 출력)


# 📘 4. 랜덤포레스트란?

**여러 개의 의사결정나무(Decision Tree)를 랜덤하게 만들어  
그 결과를 종합하여 예측하는 앙상블 학습 기법.**

하나의 트리는 과적합과 불안정성이 크지만,  
여러 트리를 결합하면 **안정적이고 정확하며 일반화 성능이 높아진다.**

---

# 5. 랜덤포레스트 핵심 원리

## (1) 부트스트랩 샘플링 (Bootstrap Sampling)  
- 전체 데이터에서 **중복 허용**하여 랜덤 샘플을 추출  
- 각 트리는 서로 다른 랜덤 데이터로 학습됨  
→ Tree1, Tree2, Tree3 … 모두 다른 관점으로 학습

## (2) 특성 랜덤 선택 (Feature Randomness)  
- 노드를 분기할 때 모든 특성을 보지 않고  
  **랜덤하게 선택된 일부 특성만 사용**  
→ 트리 간 상관성이 줄어들고 다양성이 증가

---

# 6. 예측 방식

## 분류(Classification)
- 모든 트리의 예측을 모아 **다수결 투표(Majority Voting)**

## 회귀(Regression)
- 모든 트리의 예측값을 **평균(Average)**

---

# 7. 장점

### 과적합 감소  
여러 트리를 앙상블하여 **의사결정나무의 과적합 문제 해결**

### 안정적이고 성능이 높음  
분류·회귀 모두 높은 성능

### 특성 중요도(Feature Importance) 제공  
모델 해석 가능

### 전처리 필요 거의 없음  
스케일링, 정규화 필요 없음

### 적은 튜닝에도 좋은 성능  
초보자에게 매우 친숙한 모델

---

# 8. 단점

### 개별 트리보다 계산 비용 높음  
트리를 수십~수백 개 학습해야 함

### 메모리 사용량 증가  
많은 트리를 저장해야 함

### 설명력 부족(해석 어려움)  
단일 트리는 설명 가능하지만, 숲 전체는 이해 어려움  
→ “화이트박스 → 그레이박스” 모델
