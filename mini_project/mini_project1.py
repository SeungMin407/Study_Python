# -*- coding: utf-8 -*-
"""mini_project1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OGWfZZvMJtFtrgai-BgZqN75AzcZzsUe
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
#matplotlib.use('Qt5Agg') #만약 IDLE에서 실행이 안되면 pip install pyqt5를 설치한 뒤 이 함수 실행
N, D=1000,4
X = np.random.rand(N, D) * 2  #샘플 데이터 X,y 생성
theta_gt = np.array([4, 3, 2, 1, -1])  # 초기 가중치 설정
y = theta_gt[0] + X @ theta_gt[1:] + np.random.randn(N)  # 선형 모델 + 잡음

# 선형 회귀 학습 (최적 theta 찾기)
X_c = np.c_[np.ones((N,1)), X]  # 절편 추가
theta_best = np.linalg.inv(X_c.T @ X_c) @ X_c.T @ y  # 선형 회귀 공식

# 모델 예측값 계산
y_pred_train = X_c @ theta_best  # 예측값

# MSE(Mean Squared Error) 계산
mse = np.mean((y - y_pred_train) ** 2)

print("theta_best: ",theta_best)
print("MSE: ",mse)

# 데이터 및 예측 결과 시각화 (각 특징별)
fig, axes = plt.subplots(2, 2, figsize=(10, 8))
feature_names = ["x1", "x2", "x3", "x4"]

for i, ax in enumerate(axes.flat):  # 2x2 서브플롯 루프
    ax.scatter(X[:, i], y, color="blue", alpha=0.5, label="Actual Data")  # 실제 데이터 점

    # 예측값 계산 (나머지 특징은 평균값으로 고정)
    y_pred = (
        theta_best[0]
        + theta_best[1] * (X[:, 0] if i == 0 else np.mean(X[:, 0]))
        + theta_best[2] * (X[:, 1] if i == 1 else np.mean(X[:, 1]))
        + theta_best[3] * (X[:, 2] if i == 2 else np.mean(X[:, 2]))
        + theta_best[4] * (X[:, 3] if i == 3 else np.mean(X[:, 3]))
    )

    ax.plot(X[:, i], y_pred, color="red", label="Prediction")  # 회귀선 추가

    ax.set_xlabel(feature_names[i], fontsize=12)  # x축 라벨 설정
    ax.set_ylabel("$y$", fontsize=12)  # y축 라벨 설정 (LaTeX 수식)
    ax.legend()

plt.tight_layout()
plt.show()

def mgd(X, y, initial_theta, eta=0.1, n_iterations=100, batch_size=20):
    m = len(X)
    X_c = np.c_[np.ones((m,1)), X]
    theta = initial_theta.reshape([5,1]).copy()
    theta_path = []
    for i in range(n_iterations):
        idx = np.random.choice(m, batch_size, replace=False) #중복 없이 배치 단위로 뽑기
        y = y.reshape(-1, 1)
        X_batch = X_c[idx]
        y_batch = y[idx]
        gradients = (2/batch_size) * X_batch.T @ (X_batch @ theta - y_batch)
        theta = theta - eta * gradients
        theta_path.append(theta.copy())

    return np.array(theta_path)

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
for i, ax in enumerate(axes.flat):
    for bs in [1, 10, 100, 1000]: #배치 사이즈를 변경하며 각각의 특징들을 plot
        theta_path = mgd(X, y, np.random.randn(5, 1), eta=0.1, batch_size=bs)
        ax.plot(theta_path[:, i+1, 0], theta_path[:, 0, 0], label=f"batch_size={bs}")

    ax.set_xlabel(rf"$\theta_{i+1}$")
    ax.set_ylabel(r"$\theta_0$")
    ax.set_title(rf"$\theta_0$ vs $\theta_{i+1}$")
    ax.legend()
    ax.grid(True)

plt.tight_layout()
plt.show()
print("theta_best :", theta_path[-1,:,0])

import numpy as np
import matplotlib.pyplot as plt

def gd_adaptive_learning(X, y, initial_theta, n_iterations=100, method="constant"):
    m = len(X)
    X_c = np.c_[np.ones((m, 1)), X]  # bias term 추가
    theta = initial_theta.copy()
    theta_path = []
    loss_path = []
    y = y.reshape(-1, 1)

    # optimizer 변수 초기화
    grad_squared = 0
    first_moment = 0
    second_moment = 0
    beta1 = 0.9
    beta2 = 0.999
    epsilon = 1e-7
    decay_rate = 0.9

    for i in range(1, n_iterations + 1):
        gradients = (2/m) * X_c.T @ (X_c @ theta - y)

        if method == "constant":
            eta = 0.1
            theta -= eta * gradients

        elif method == "adagrad":
            grad_squared += gradients ** 2
            theta -= 0.1 * gradients / (np.sqrt(grad_squared) + epsilon)

        elif method == "rmsprop":
            grad_squared = decay_rate * grad_squared + (1 - decay_rate) * gradients ** 2
            theta -= 0.01 * gradients / (np.sqrt(grad_squared) + epsilon)

        elif method == "adam":
            first_moment = beta1 * first_moment + (1 - beta1) * gradients
            second_moment = beta2 * second_moment + (1 - beta2) * gradients ** 2
            first_unbias = first_moment / (1 - beta1 ** i)
            second_unbias = second_moment / (1 - beta2 ** i)
            theta -= 0.01 * first_unbias / (np.sqrt(second_unbias) + epsilon)

        theta_path.append(theta.copy())
        loss = np.mean((X_c @ theta - y) ** 2)
        loss_path.append(loss)

    return np.array(theta_path), np.array(loss_path)

# 시각화
plt.figure(figsize=(12, 7))
initial_weight = np.random.randn(X.shape[1]+1, 1)

methods = ["constant", "adagrad", "rmsprop", "adam"]

for method in methods:
    _, loss_path = gd_adaptive_learning(X, y, initial_weight, method=method)
    plt.plot(loss_path, label=method)

plt.xlabel("Iteration")
plt.ylabel("Loss (MSE)")
plt.title("Loss Curve for Different Learning Rate Strategies")
plt.legend()
plt.grid(True)
plt.show()

def init_small_random(n):
    return np.random.randn(n, 1) * 0.01

def init_xavier(n):
    return np.random.randn(n, 1) * np.sqrt(2 / n)

def init_random(n):
    return np.random.randn(n, 1)

plt.figure(figsize=(12, 7))
initializers = {
    "Xavier": init_xavier,
    "Small Random": init_small_random,
    "Random" : init_random
}

for name, init_func in initializers.items():
    init_theta = init_func(X.shape[1] + 1)  # weight shape is (5,1)
    _, loss_path = gd_adaptive_learning(X, y, init_theta, n_iterations=200, method="adam")
    plt.plot(loss_path, label=name)

plt.xlabel("Iteration")
plt.ylabel("Loss (MSE)")
plt.title("Loss Curve for Different Weight Initializations")
plt.legend()
plt.grid(True)
plt.show()
