# -*- coding: utf-8 -*-
"""mini_project2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUIxUXjo_QeHKZbGXl7uWD6Gtl1tuM6i
"""

import numpy as np
import matplotlib.pyplot as plt
#matplotlib.use('Qt5Agg') #만약 IDLE에서 실행이 안되면 pip install pyqt5를 설치한 뒤 이 함수 실행
N, D=1000,4
p1 = 0.9
n_w1 = int(N * p1)
n_w2 = N - n_w1

theta_w1 = np.array([1.0, 2.0, -1.5, 3.0, 0.5])  # 실제 데이터 가중치
theta_w2 = np.array([5.0, -3.0, 2.0, -2.0, 1.0]) # 이상치 가중치

X = 2 * np.random.rand(N, D)

# y 생성 : y
X_w1 = X[:n_w1]
X_w2 = X[n_w1:]
y_w1 = theta_w1[0] + X_w1 @ theta_w1[1:].reshape(-1, 1) + np.random.randn(n_w1, 1)
y_w2 = theta_w2[0] + X_w2 @ theta_w2[1:].reshape(-1, 1) + np.random.randn(n_w2, 1)

# linear regression : w1 데이터만 사용
X_w1_c = np.c_[np.ones((n_w1, 1)), X_w1]
theta_best = np.linalg.inv(X_w1_c.T @ X_w1_c) @ X_w1_c.T @ y_w1

# 예측값 및 MSE
y_pred_train = X_w1_c @ theta_best
mse = np.mean((y_w1 - y_pred_train) ** 2)
print("Estimated theta (ideal case using w1 data only) :", theta_best.reshape(5))
print("MSE :", mse)

# 시각화
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.ravel() #axes를 1차원 배열로
feature_names = ['x1', 'x2', 'x3', 'x4']

for i in range(D):
    ax = axes[i]

    # w1 데이터 산점도 (파란 점)
    ax.scatter(X_w1[:, i], y_w1, color='blue', alpha=0.4, label="y from w1")

    # w2 데이터 산점도 (빨간 점)
    ax.scatter(X_w2[:, i], y_w2, color='red', alpha=0.6, marker='x', label="y from w2")

    # 예측 선 그리기 (다른 feature는 평균값 고정)
    x_vals = np.linspace(0, 2, 100) #x축 범위
    other_avg = np.mean(X_w1, axis=0)
    X_fixed = np.tile(other_avg, (100, 1)) #평균을 취한 특성들을 세로방향으로 복제
    X_fixed[:, i] = x_vals #plot하려는 특성 부분만 바꿔줌
    X_plot_c = np.c_[np.ones((100, 1)), X_fixed]
    y_line = X_plot_c @ theta_best

    ax.plot(x_vals, y_line, 'k--', linewidth=2, label="Predicted Line (w1 only)")

    ax.set_xlabel(feature_names[i])
    ax.set_ylabel("y")
    ax.legend()
    ax.grid(True)

plt.suptitle("Ideal Case", fontsize=14)
plt.tight_layout()
plt.show()

#solution 2-2
import numpy as np
import matplotlib.pyplot as plt

N, D=1000,4
p1 = 0.9
n_w1 = int(N * p1)
n_w2 = N - n_w1

theta_w1 = np.array([1.0, 2.0, -1.5, 3.0, 0.5])
theta_w2 = np.array([5.0, -3.0, 2.0, -2.0, 1.0]) # 이상치용 가중치

X = 2 * np.random.rand(N, D)

# y 생성
X_w1 = X[:n_w1]
X_w2 = X[n_w1:]
y_w1 = theta_w1[0] + X_w1 @ theta_w1[1:].reshape(-1, 1) + np.random.randn(n_w1, 1)
y_w2 = theta_w2[0] + X_w2 @ theta_w2[1:].reshape(-1, 1) + np.random.randn(n_w2, 1)
y_sum = np.concatenate((y_w1, y_w2), axis=0)
# 회귀
X_c = np.c_[np.ones((N, 1)), X]
theta_est = np.linalg.inv(X_c.T @ X_c) @ X_c.T @ y_sum

X_w1_c = np.c_[np.ones((n_w1, 1)), X_w1]
theta_est_w1 = np.linalg.inv(X_w1_c.T @ X_w1_c) @ X_w1_c.T @ y_w1

# 예측값 및 MSE
y_pred_train = X_c @ theta_est
mse = np.mean((y_sum - y_pred_train) ** 2)
print("Estimated theta :", theta_best.reshape(5))
print("MSE :", mse)

# 시각화
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.ravel() #axes를 1차원 배열로
feature_names = ['x1', 'x2', 'x3', 'x4']

for i in range(D):
    ax = axes[i]

    # w1 데이터 산점도 (파란 점)
    ax.scatter(X_w1[:, i], y_w1, color='blue', alpha=0.4, label="y from w1")

    # w2 데이터 산점도 (빨간 점)
    ax.scatter(X_w2[:, i], y_w2, color='red', alpha=0.6, marker='x', label="y from w2")

    # 예측 선 그리기 (다른 feature는 평균값 고정)
    x_vals = np.linspace(0, 2, 100)
    other_avg = np.mean(X, axis=0)
    X_fixed = np.tile(other_avg, (100, 1))
    X_fixed[:, i] = x_vals
    X_plot_c = np.c_[np.ones((100, 1)), X_fixed]
    y_line = X_plot_c @ theta_est

    other_avg_w1 = np.mean(X_w1, axis=0) #기존 w1만 쓴 예측선 그리기
    X_fixed_w1 = np.tile(other_avg_w1, (100, 1))
    X_fixed_w1[:, i] = x_vals
    X_plot_c_w1 = np.c_[np.ones((100, 1)), X_fixed_w1]
    y_line_w1 = X_plot_c_w1 @ theta_est_w1

    ax.plot(x_vals, y_line, 'k-', linewidth=2,label="Predicted Line")
    ax.plot(x_vals, y_line_w1, 'k--', linewidth=2,label="Predicted Line_w1")

    ax.set_xlabel(feature_names[i])
    ax.set_ylabel("y")
    ax.legend()
    ax.grid(True)

plt.suptitle("Practical Case", fontsize=14)
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def gradient_descent_anlysis(X, y, theta, eta, feature_names=None):
    theta_path = []

    n_iterations = 100
    m, n_features = X.shape
    X_c = np.c_[np.ones((m, 1)), X]  # bias 포함

    if feature_names is None:
        feature_names = [f"$x_{i+1}$" for i in range(n_features)]

    # 시각화 준비
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))

    # subplot용 feature별 산점도 미리 그리기
    for i, ax in enumerate(axes.flat):
        # w1 데이터 산점도 (파란 점)
        ax.scatter(X_w1[:, i], y_w1, color='blue', alpha=0.4, label="y from w1")

        # w2 데이터 산점도 (빨간 점)
        ax.scatter(X_w2[:, i], y_w2, color='red', alpha=0.6, marker='x', label="y from w2")
        ax.grid(True)
        ax.set_xlabel(feature_names[i])
        ax.set_ylabel("y")

    # Gradient Descent 수행하며 선 이동방향 그리기
    for iteration in range(n_iterations):
        gradients = 2/m * X_c.T @ (X_c @ theta - y)
        theta = theta - eta * gradients
        theta_path.append(theta.copy())

        # 각 subplot에 회귀선 그리기
        for i, ax in enumerate(axes.flat):
            # 해당 feature는 x축으로 유지, 나머지는 평균 고정
            X_temp = np.full_like(X, np.mean(X, axis=0))
            X_temp[:, i] = X[:, i]
            X_temp_c = np.c_[np.ones((m, 1)), X_temp]
            y_pred = X_temp_c @ theta

            # 선 그리기
            if iteration == 0:
                ax.plot(X[:, i], y_pred, 'g--', label="Start")
            elif iteration < 10:
                ax.plot(X[:, i], y_pred, 'r', alpha=0.3)

    # 범례 추가 및 타이틀
    for ax in axes.flat:
        ax.legend()

    plt.suptitle("Gradient Descent", fontsize=16)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

    return theta_path



theta_path_gd = gradient_descent_anlysis( X, y_sum, np.random.randn(5,1), eta=0.1)

import numpy as np
import matplotlib.pyplot as plt

def mgd(X, y, initial_theta, eta=0.1, n_iterations=100, batch_size=20):
    m = len(X)
    X_c = np.c_[np.ones((m,1)), X]
    theta = initial_theta.reshape([5,1]).copy()
    y = y.reshape(-1, 1)
    loss_path = []

    for i in range(n_iterations):
        idx = np.random.choice(m, batch_size, replace=False)
        X_batch = X_c[idx]
        y_batch = y[idx]
        gradients = (2/batch_size) * X_batch.T @ (X_batch @ theta - y_batch)
        theta = theta - eta * gradients

        # MSE계산
        y_pred = X_c @ theta
        mse = np.mean((y_pred - y) ** 2)
        loss_path.append(mse)

    return loss_path

# 배치 사이즈들에 따른 loss 곡선 그리기
batch_sizes = [1, 10, 100, 1000]
plt.figure(figsize=(10, 6))

for bs in batch_sizes:
    loss_curve = mgd(X, y_sum, np.random.randn(5, 1), eta=0.1, batch_size=bs)
    plt.plot(loss_curve, label=f"batch_size={bs}")

plt.xlabel("Iteration")
plt.ylabel("MSE Loss")
plt.title("Loss curve according to batch size")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def gd_adaptive_learning(X, y, initial_theta, n_iterations=100, method="constant"):
    m = len(X)
    X_c = np.c_[np.ones((m, 1)), X]  # bias term 추가
    theta = initial_theta.copy()
    theta_path = []
    loss_path = []
    y = y.reshape(-1, 1)

    # optimizer 변수 초기화
    grad_squared = 0
    first_moment = 0
    second_moment = 0
    beta1 = 0.9
    beta2 = 0.999
    epsilon = 1e-7
    decay_rate = 0.9

    for i in range(1, n_iterations + 1):
        gradients = (2/m) * X_c.T @ (X_c @ theta - y)

        if method == "constant":
            eta = 0.1
            theta -= eta * gradients

        elif method == "adagrad":
            grad_squared += gradients ** 2
            theta -= 0.1 * gradients / (np.sqrt(grad_squared) + epsilon)

        elif method == "rmsprop":
            grad_squared = decay_rate * grad_squared + (1 - decay_rate) * gradients ** 2
            theta -= 0.01 * gradients / (np.sqrt(grad_squared) + epsilon)

        elif method == "adam":
            first_moment = beta1 * first_moment + (1 - beta1) * gradients
            second_moment = beta2 * second_moment + (1 - beta2) * gradients ** 2
            first_unbias = first_moment / (1 - beta1 ** i)
            second_unbias = second_moment / (1 - beta2 ** i)
            theta -= 0.01 * first_unbias / (np.sqrt(second_unbias) + epsilon)

        theta_path.append(theta.copy())
        loss = np.mean((X_c @ theta - y) ** 2)
        loss_path.append(loss)

    return np.array(theta_path), np.array(loss_path)

# 시각화
plt.figure(figsize=(12, 7))
initial_weight = np.random.randn(X.shape[1]+1, 1)

methods = ["constant", "adagrad", "rmsprop", "adam"]

for method in methods:
    _, loss_path = gd_adaptive_learning(X, y_sum, initial_weight, method=method)
    plt.plot(loss_path, label=method)

plt.xlabel("Iteration")
plt.ylabel("Loss (MSE)")
plt.title("Loss Curve for Different Learning Rate Strategies")
plt.legend()
plt.grid(True)
plt.show()

def init_small_random(n):
    return np.random.randn(n, 1) * 0.01

def init_xavier(n):
    return np.random.randn(n, 1) * np.sqrt(2 / n)

def init_random(n):
    return np.random.randn(n, 1)

plt.figure(figsize=(12, 7))
initializers = {
    "Xavier": init_xavier,
    "Small Random": init_small_random,
    "Random" : init_random
}

for name, init_func in initializers.items():
    init_theta = init_func(X.shape[1] + 1)  # weight shape is (5,1)
    _, loss_path = gd_adaptive_learning(X, y_sum, init_theta, n_iterations=200, method="adam")
    plt.plot(loss_path, label=name)

plt.xlabel("Iteration")
plt.ylabel("Loss (MSE)")
plt.title("Loss Curve for Different Weight Initializations")
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

N, D=1000,4
p = [0.6, 0.7, 0.8, 0.9, 1]
theta_w1 = np.array([1.0, 2.0, -1.5, 3.0, 0.5])
theta_w2 = np.array([5.0, -3.0, 2.0, -2.0, 1.0]) # 이상치용 가중치

X = 2 * np.random.rand(N, D)
initial_weight = np.random.randn(D+1, 1)
for method in ["constant", "adam"]: #두 가지 방법으로 각각 그리기
  plt.figure(figsize=(12, 7))
  for p1 in p: #각 확률에 대해 loss 곡선 그리기
    n_w1 = int(N * p1)
    n_w2 = N - n_w1

    # y 생성
    X_w1 = X[:n_w1]
    X_w2 = X[n_w1:]
    y_w1 = theta_w1[0] + X_w1 @ theta_w1[1:].reshape(-1, 1) + np.random.randn(n_w1, 1)
    y_w2 = theta_w2[0] + X_w2 @ theta_w2[1:].reshape(-1, 1) + np.random.randn(n_w2, 1)
    y_sum = np.concatenate((y_w1, y_w2), axis=0)
    _, loss_path = gd_adaptive_learning(X, y_sum, initial_weight, method=method)
    plt.plot(loss_path, label=p1)

  plt.xlabel("Iteration")
  plt.ylabel("Loss (MSE)")
  plt.title(f"Loss Curve for Different P1 with methods: {method}")
  plt.legend()
  plt.grid(True)
  plt.show()

def mgd_adam(X, y, initial_theta, eta=0.01, n_iterations=100, batch_size=20):
    m = len(X)
    X_c = np.c_[np.ones((m, 1)), X]  # bias term 추가
    theta = initial_theta.reshape(-1, 1).copy()
    y = y.reshape(-1, 1)
    loss_path = []

    # Adam optimizer 변수 초기화
    grad_squared = 0
    first_moment = 0
    second_moment = 0
    beta1 = 0.9
    beta2 = 0.999
    epsilon = 1e-7

    for i in range(1, n_iterations + 1):
        # 미니배치 선택
        idx = np.random.choice(m, batch_size, replace=False)
        X_batch = X_c[idx]
        y_batch = y[idx]

        # gradient 계산
        gradients = (2 / batch_size) * X_batch.T @ (X_batch @ theta - y_batch)

        # Adam 방식 적용
        first_moment = beta1 * first_moment + (1 - beta1) * gradients
        second_moment = beta2 * second_moment + (1 - beta2) * gradients ** 2
        first_unbias = first_moment / (1 - beta1 ** i)
        second_unbias = second_moment / (1 - beta2 ** i)
        theta -= eta * first_unbias / (np.sqrt(second_unbias) + epsilon)

        # MSE 계산
        y_pred = X_c @ theta
        mse = np.mean((y_pred - y) ** 2)
        loss_path.append(mse)

    return loss_path
initial_weight = np.random.randn(D+1, 1)
loss_mgd_adam=mgd_adam(X,y_sum,initial_weight, n_iterations=200, batch_size=100)
_, loss_path = gd_adaptive_learning(X, y_sum, initial_weight, n_iterations=200, method="constant")
plt.figure(figsize=(12, 7))
plt.plot(loss_mgd_adam, label="adam, mgd")
plt.plot(loss_path, label="constant, gd")
plt.xlabel("Iteration")
plt.ylabel("Loss (MSE)")
plt.title(f"Loss Curve considering performance improvement")
plt.legend()
plt.grid(True)
plt.show()
